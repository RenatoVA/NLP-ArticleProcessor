{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SYCL_CACHE_PERSISTENT\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "from langchain_community.embeddings import IpexLLMBgeEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader('downloaded_documents/gous2.pdf')\n",
    "paginas = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paginas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size=650,\n",
    "    chunk_overlap=80,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=text_splitter.split_documents(paginas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\renat\\miniconda3\\envs\\llm\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\renat\\miniconda3\\envs\\llm\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\renat\\miniconda3\\envs\\llm\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-07-12 23:50:26,915 - INFO - intel_extension_for_pytorch auto imported\n",
      "2024-07-12 23:50:26,916 - INFO - Load pretrained SentenceTransformer: google/flan-t5-large\n",
      "2024-07-12 23:50:27,404 - WARNING - No sentence-transformers model found with name google/flan-t5-large. Creating a new one with mean pooling.\n",
      "c:\\Users\\renat\\miniconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-07-12 23:50:29,111 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\n",
      "2024-07-12 23:50:29,165 - INFO - Only HuggingFace Transformers models are currently supported for further optimizations\n",
      "2024-07-12 23:50:30,027 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2a214ba6f941ab8aba5a34dde31af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = IpexLLMBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"xpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    "    query_instruction=\"Identify and extract the names of the main countries studied or focused on in this research\",\n",
    ")\n",
    "vectorstore=Chroma.from_documents(documents=docs, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_llm(question,context):\n",
    "    formatted_question = f\"Question: {question}\\n\\nContext (Answer only about the content of the provided context):{context}\"\n",
    "    response =ollama.chat(model='llama3:8b',\n",
    "                          messages=[{\"role\": \"user\", \"content\": formatted_question}],\n",
    "                          options={'temperature':0})\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever()\n",
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context=combine_docs(retrieved_docs)\n",
    "    return ollama_llm(question,formatted_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f226635afc24290a19bc871afb8abd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 23:50:42,813 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result=rag_chain(\"In which countries was the research presented in the past texr carried outt ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research presented was carried out in Guinea-Bissau, as mentioned in the text: \"a school ‐based survey in Guinea ‐Bissau found that one ‐ third of adolescents in secondary schools used mobile internet daily...\" (Gunnlaugsson et al., 2020).\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_embeddings[0][:10]: [0.035797119140625, 0.032745361328125, -0.0166778564453125, 0.007411956787109375, 0.016265869140625, -0.0019683837890625, -0.0028476715087890625, -0.041412353515625, 0.0310211181640625, 0.054412841796875]\n",
      "text_embeddings[1][:10]: [0.031524658203125, 0.031768798828125, -0.0030384063720703125, 0.004299163818359375, 0.0049896240234375, -0.02679443359375, -0.0058441162109375, -0.022491455078125, 0.0516357421875, 0.059295654296875]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding[:10]: [0.035614013671875, 0.0299530029296875, 0.005218505859375, 0.005504608154296875, 0.0092926025390625, -0.0364990234375, -0.0203094482421875, -0.0355224609375, 0.03375244140625, 0.0633544921875]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = \"IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency.\"\n",
    "query = \"What is IPEX-LLM?\"\n",
    "\n",
    "text_embeddings = embedding_model.embed_documents([sentence, query])\n",
    "print(f\"text_embeddings[0][:10]: {text_embeddings[0][:10]}\")\n",
    "print(f\"text_embeddings[1][:10]: {text_embeddings[1][:10]}\")\n",
    "\n",
    "query_embedding = embedding_model.embed_query(query)\n",
    "print(f\"query_embedding[:10]: {query_embedding[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
